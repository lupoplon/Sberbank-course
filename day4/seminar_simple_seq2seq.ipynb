{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgH3CAcubM4z"
   },
   "source": [
    "# Семинар на тему Seq2Seq модели в машинном переводе\n",
    "\n",
    "На лекции мы подробно познакомились с подходами к решению задачи машинного перевода.\n",
    "Наиболее распространенными моделями последовательностей (seq2seq) являются модели кодер-декодер, которые (обычно) используют рекуррентную нейронную сеть (RNN) для кодирования исходного (входного) предложения в один вектор.  Вы можете думать о векторе контекста как об абстрактном представлении всего входного предложения. Этот вектор затем декодируется декодером, который учится выводить  предложение, генерируя его по одному слову за раз.\n",
    "\n",
    "$h_t = \\text{Encoder}(x_t, h_{t-1})$\n",
    "\n",
    "У нас есть последовательность $X = \\{x_1, x_2, ..., x_T\\}$, где $x_1 = \\text{<sos>;}, x_2 = \\text{the}$, и так далее. Начальное состояние, $h_0$,  может быть инициализировано вектором из нулей или обучаемым.\n",
    "\n",
    "\n",
    "Как только последнее слово, $x_T$, был подан на Encoder, мы  используем  информацию в  последнем скрытом состоянии, $h_T$, в зависимости от контекста вектор, т. е. $h_T $ это векторное представление всего исходного предложения.\n",
    "\n",
    "После получения вектора всего предложения мы можем декодировать предложение уже на новом языке. На каждом шаге декодирования мы подаем правильное слово $y_t$,  дополняем это информацией о скрытом состоянии $s_{t-1}$, где  $s_t = \\text{DecoderRNN}(y_t, s_{t-1})$\n",
    "\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/f6DQb.png)\n",
    "\n",
    "\n",
    "Мы всегда используем $<sos>$ для первого входа в декодер, $y_1$, но для последующих входов, $y_{\\text{from }t; 1}$, мы иногда будем использовать фактическое, основное истинное следующее слово в последовательности, $y_t$, а иногда использовать слово, предсказанное нашим декодером, $\\hat{y}_{t-1}$. Использование настоящих токенов в декодере называется Teacher Forcing [можно тут посмотреть](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/)\n",
    "\n",
    "Мы будем  использовать TorchText и spaCy( как токенизатор) , чтобы помочь вам выполнить всю необходимую предварительную обработку быстрее чем мы делали раньше. В данной работе предлагается написать модель Seq2Seq и обучить ее на Multi30k. В данном задание мы будем подавать на вход перевернутые предложения, так как авторы seq2seq считали, что это улучшает качество перевода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fPuwHEnVIzn"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import spacy\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uQSnhb84VLU7"
   },
   "outputs": [],
   "source": [
    "seed = 43\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 534
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11573,
     "status": "ok",
     "timestamp": 1555777705100,
     "user": {
      "displayName": "Deep Learning School",
      "photoUrl": "https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg",
      "userId": "16549096980415837553"
     },
     "user_tz": -180
    },
    "id": "L10vdpVaVXBo",
    "outputId": "c00ca813-4579-4173-cd67-da8ef637f2ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\n",
      "\u001b[K     |################################| 11.1MB 50.3MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-r8i4f31a/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.1.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m[+] Linking successful\u001b[0m\n",
      "/opt/miniconda/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "/opt/miniconda/lib/python3.6/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Collecting de_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.1.0/de_core_news_sm-2.1.0.tar.gz#egg=de_core_news_sm==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.1.0/de_core_news_sm-2.1.0.tar.gz (11.1MB)\n",
      "\u001b[K     |################################| 11.1MB 944kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: de-core-news-sm\n",
      "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-5snh6d1l/wheels/b4/8b/5e/d2ce5d2756ca95de22f50f68299708009a4aafda2aea79c4e4\n",
      "Successfully built de-core-news-sm\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-2.1.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m[+] Linking successful\u001b[0m\n",
      "/opt/miniconda/lib/python3.6/site-packages/de_core_news_sm -->\n",
      "/opt/miniconda/lib/python3.6/site-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en\n",
    "! python -m spacy download de\n",
    "\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MV8d8onrbN4W"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ferOqkOUVirW"
   },
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings (tokens)\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "# немецкий язык является полем SRC, а английский в поле TRG\n",
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11840,
     "status": "ok",
     "timestamp": 1555778026395,
     "user": {
      "displayName": "Deep Learning School",
      "photoUrl": "https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg",
      "userId": "16549096980415837553"
     },
     "user_tz": -180
    },
    "id": "u6pNY6cWW3j5",
    "outputId": "569ded5a-29c0-440f-e763-d166a903f66b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading training.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:00<00:00, 1.29MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading validation.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 453kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading mmt_task1_test2016.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 393kB/s]\n"
     ]
    }
   ],
   "source": [
    "# В датасете содержится ~ 30к предложений средняя длина которых 11\n",
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),  fields = (SRC, TRG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOS3e7QZbLro"
   },
   "source": [
    "Давайте посмотрим что у нас с датасетом и сделаем словари для SRC и TGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1555778067129,
     "user": {
      "displayName": "Deep Learning School",
      "photoUrl": "https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg",
      "userId": "16549096980415837553"
     },
     "user_tz": -180
    },
    "id": "r0Xpf4IBW4Uf",
    "outputId": "06525284-2b51-4724-aa8e-d03bc2afd730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train : 29000\n",
      "Number of sentences in validation : 1014\n",
      "Number of sentences in test : 1000\n"
     ]
    }
   ],
   "source": [
    "labels = ['train', 'validation', 'test']\n",
    "dataloaders = [train_data, valid_data, test_data]\n",
    "for d, l in zip(dataloaders, labels):\n",
    "    print(\"Number of sentences in {} : {}\".format(l, len(d.examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 682,
     "status": "ok",
     "timestamp": 1555778111414,
     "user": {
      "displayName": "Deep Learning School",
      "photoUrl": "https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg",
      "userId": "16549096980415837553"
     },
     "user_tz": -180
    },
    "id": "Gg63m8haW4XC",
    "outputId": "9a138b06-328a-41f0-f2ef-b559b439fede"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in source vocabulary 7855\n",
      "Number of words in source vocabulary 5893\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)\n",
    "print(\"Number of words in source vocabulary\", len(SRC.vocab))\n",
    "print(\"Number of words in source vocabulary\", len(TRG.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 444,
     "status": "ok",
     "timestamp": 1555781459937,
     "user": {
      "displayName": "Deep Learning School",
      "photoUrl": "https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg",
      "userId": "16549096980415837553"
     },
     "user_tz": -180
    },
    "id": "-FhP6bJOzASS",
    "outputId": "70ea4ded-6178-4a81-dd05-bb673907bba3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "            2,    2,    2,    2,    2,    2,    2,    2],\n",
       "        [   0, 3833,    0,  664,    0,    0, 5886, 3833,    0,  664,    0, 5886,\n",
       "         7098,    0, 5886,    0, 7098,    0, 5886, 7011],\n",
       "        [   3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC.process(\"Ein klein gespenster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LSd3la5FbJ5_"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "Напишем для начала простой Encoder, который реализует следующий функционал:\n",
    "\n",
    "$ (h_t, c_t) = \\text{LSTM}(x_t, (h_{t-1}, c_{t-1}))$\n",
    "\n",
    "В  методе forward мы передаем исходное предложение $X$, которое преобразуется в embeddings, к которым применяется dropout . Эти вектора затем передаются в RNN. Когда мы передадим всю последовательность RNN, он автоматически выполнит для нас рекуррентный расчет скрытых состояний по всей последовательности! Вы можете заметить, что мы не передаем начальное скрытое или состояние ячейки в RNN. Это происходит потому, что, как отмечено в документации, если никакое скрытое состояние/ячейка не передается RNN, он автоматически создаст начальное скрытое состояние/ячейка как тензор всех нулей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Ar5SN6tW4ck"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        \"\"\"\n",
    "        :param: input_dim is the size/dimensionality of the one-hot vectors that will be input to the encoder. This is equal to the input (source) vocabulary size.\n",
    "        :param: emb_dim is the dimensionality of the embedding layer. This layer converts the one-hot vectors into dense vectors with emb_dim dimensions.\n",
    "        :param: hid_dim is the dimensionality of the hidden and cell states.\n",
    "        :param: n_layers is the number of layers in the RNN.\n",
    "        :param: percentage of the dropout to use\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        :param: src sentences (src_len x batch_size)\n",
    "        \"\"\"\n",
    "        # embedded = <TODO> (src_len x batch_size x embd_dim)\n",
    "        embedded = self.embedding(src)\n",
    "        # dropout over embedding\n",
    "        embedded = self.dropout(embedded)\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # [Attention return is for lstm, but you can also use gru]\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8QOCpKxfD3M"
   },
   "source": [
    "## Decoder\n",
    "Похожий на Encoder, но со слоем проекцией, который переводит из hidden_dim в output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gRgtzaf4bJp6"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.output_dim, self.emb_dim)\n",
    "        self.rnn = nn.LSTM(self.emb_dim, self.hid_dim, self.n_layers) #(lstm embd, hid, layers, dropout)\n",
    "        self.out = nn.Linear(self.hid_dim, self.output_dim)# Projection :hid_dim x output_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_, hidden, cell):\n",
    "        \n",
    "        # (1x batch_size)\n",
    "        input_ = input_.unsqueeze(0)\n",
    "        \n",
    "        # (1 x batch_size x emb_dim)\n",
    "        embedded = self.embedding(input_)# embd over input and dropout \n",
    "        embedded = self.dropout(embedded)\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #sent len and n directions will always be 1 in the decoder\n",
    "        # (batch_size x output_dim)\n",
    "        \n",
    "        prediction = self.out(output.squeeze(0)) #project out of the rnn on the output dim \n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UlD7-nusfL86"
   },
   "source": [
    "## Seq2Seq module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_YvVGzaW4fY"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        # Hidden dimensions of encoder and decoder must be equal\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self._init_weights()  \n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \"\"\"\n",
    "        :param: src (src_len x batch_size)\n",
    "        :param: tgt\n",
    "        :param: teacher_forcing_ration : if 0.5 then every second token is the ground truth input\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src) # TODO pass src throw encoder\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :] # trg[idxs]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            \n",
    "            output, hidden, cell = self.decoder(input, hidden, cell) #TODO pass state and input throw decoder \n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = (trg[t] if teacher_force else top1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        p = 0.08\n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.uniform_(param.data, -p, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msbn2VypfUur"
   },
   "outputs": [],
   "source": [
    "input_dim = len(SRC.vocab)\n",
    "output_dim = len(TRG.vocab)\n",
    "src_embd_dim =  tgt_embd_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers =  2\n",
    "dropout_prob = 0.2\n",
    "\n",
    "batch_size = 128\n",
    "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "\n",
    "iterators = BucketIterator.splits((train_data, valid_data, test_data),\n",
    "                                  batch_size = batch_size, device = device)\n",
    "train_iterator, valid_iterator, test_iterator = iterators\n",
    "\n",
    "enc = Encoder(input_dim, src_embd_dim, hidden_dim, num_layers, dropout_prob)\n",
    "dec = Decoder(output_dim, tgt_embd_dim, hidden_dim, num_layers, dropout_prob)\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1555780300545,
     "user": {
      "displayName": "Deep Learning School",
      "photoUrl": "https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg",
      "userId": "16549096980415837553"
     },
     "user_tz": -180
    },
    "id": "h5V9ZnK4fUxq",
    "outputId": "4114497e-a04a-4276-9555-f79ef2d0bcd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7855, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2)\n",
       "    (dropout): Dropout(p=0.2)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2)\n",
       "    (out): Linear(in_features=512, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.2)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vaUeDjTfU4k"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        \n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "        \n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HfbTx2FMjaIM"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in tqdm(enumerate(iterator)):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing !!\n",
    "\n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 399836,
     "status": "ok",
     "timestamp": 1555781004652,
     "user": {
      "displayName": "Deep Learning School",
      "photoUrl": "https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg",
      "userId": "16549096980415837553"
     },
     "user_tz": -180
    },
    "id": "lQV_yqkLjcyQ",
    "outputId": "3cd40bc5-2654-452a-e01d-6e6abc9acc0c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820a9e6a08004907850ef916e2d06fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f772c506a3354d75811a31e1790e5e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0 \n",
      " Train Loss 5.04188  Val loss 4.95822:\n",
      "Train Perplexity 154.7606918046976  Val Perplexity 142.34020470341125:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4650b6f72f2e49c593d5ca5e3f39ef7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5385e93f0d7d4da9a98c259ca9677356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 \n",
      " Train Loss 4.46208  Val loss 4.76833:\n",
      "Train Perplexity 86.66759033481252  Val Perplexity 117.72248116758739:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc6292976d043fca1d07f57d0a9c82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfffc1f65c142db99ee63ce4a182b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2 \n",
      " Train Loss 4.13285  Val loss 4.5746:\n",
      "Train Perplexity 62.355382775279516  Val Perplexity 96.98923568248686:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7778664daf4bd1bbe4881270a9e641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d200f39a11ff4926b5a33745a9fcabaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3 \n",
      " Train Loss 3.8805  Val loss 4.38105:\n",
      "Train Perplexity 48.4484332329167  Val Perplexity 79.92190736628358:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9969fe5c609456a815dc9d39d55d1d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8017f7624c040d0bb4df99573ddbbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4 \n",
      " Train Loss 3.69466  Val loss 4.25351:\n",
      "Train Perplexity 40.23189141922221  Val Perplexity 70.3519147025816:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd87efa2f997455ebe088c8ca2597a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01c551bee0a4950bdb126d81b27431c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5 \n",
      " Train Loss 3.51915  Val loss 4.08113:\n",
      "Train Perplexity 33.755723900822815  Val Perplexity 59.21234200646534:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7deb4bd37f049b3a4ba99117f604566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f9915f79234c7f84eb7538bcc35ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6 \n",
      " Train Loss 3.33107  Val loss 4.12978:\n",
      "Train Perplexity 27.96825172786848  Val Perplexity 62.16424529631056:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfcb06f63f654d5aa44aa928c76ef3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ecfe466705a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-0ba657762e60>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "CLIP = 1\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    train_loss = round(train(model, train_iterator, optimizer, criterion, CLIP), 5)\n",
    "    valid_loss = round(evaluate(model, valid_iterator, criterion),5)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "    \n",
    "    print('Epoch: {} \\n Train Loss {}  Val loss {}:'.format(epoch, train_loss, valid_loss))\n",
    "    print('Train Perplexity {}  Val Perplexity {}:'.format(np.exp(train_loss), np.exp(valid_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 32
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 970,
     "status": "ok",
     "timestamp": 1555781269111,
     "user": {
      "displayName": "Deep Learning School",
      "photoUrl": "https://lh6.googleusercontent.com/-pCAqm81DR_s/AAAAAAAAAAI/AAAAAAAAAMk/4cGT0s91JQs/s64/photo.jpg",
      "userId": "16549096980415837553"
     },
     "user_tz": -180
    },
    "id": "n5Zf6Kb1jhOI",
    "outputId": "46edd4a4-9cc9-49cc-c4e3-fbd9d01cb68b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87871f6a420d467daac41687e27f2f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| Test Loss: 4.154056072235107 Test PPL:63.69181569096384|\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print('| Test Loss: {} Test PPL:{}|'.format(test_loss, np.exp(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "an6HG7_uyjJN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "simple_seq2seq_seminar.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
