{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1IMoqFPZ-_E"
   },
   "source": [
    "<!-- #  Семинар: Рекурентные нейронные сети. -->\n",
    "\n",
    "В данной работе вам предлагается посмотреть на всю мощь рекурентных нейронных сетей решив небольшую задачу. \n",
    "\n",
    " Предлагаю решить вам задачу расшифровки сообщения с помощью RNN. \n",
    " Представьте, что вам даны сообщения зашифрованные с помощью шифра Цезаря, являющимся одним из самый простых шифров в криптографии.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YNGZnxua61K"
   },
   "source": [
    "Шифр цезаря работает следующим образом: каждая буква \n",
    "исходного алфавита сдвигается на K символов вправо: \n",
    "\n",
    "Пусть нам дано сообщение: message=\"RNN IS NOT AI\", тогда наше шифрование выполняющиеся по правилу f, с K=2, даст нам результат:\n",
    "f(message, K) = TPPAKUAPQVACK\n",
    "\n",
    "Для удобство можно взять символы только одного регистра в нашей имплементации, и сказать, что все буквы не английского алфавита будут отмечены как прочерк \"-\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPH9sAiDe77A"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xoZVmCuEdO7Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '-', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
     ]
    }
   ],
   "source": [
    "# Определим ключ и словарь\n",
    "key = 2\n",
    "\n",
    "vocab = [char for char in ' -ABCDEFGHIJKLMNOPQRSTUVWXYZ']\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1189,
     "status": "ok",
     "timestamp": 1553354624454,
     "user": {
      "displayName": "Агата Шевченко",
      "photoUrl": "",
      "userId": "17622558543368228675"
     },
     "user_tz": -180
    },
    "id": "yAsJDtSsGgOV",
    "outputId": "26271074-0e8a-43b4-8e1a-7320e080e3bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPPAKUAPQVACK\n"
     ]
    }
   ],
   "source": [
    "# Напишем функцию, которая делает \n",
    "def encrypt(text, key):\n",
    "    \"\"\"Returns the encrypted form of 'text'.\"\"\"\n",
    "    indexes = [vocab.index(char) for char in text]\n",
    "    encrypted_indexes = [(idx + key) % len(vocab) for idx in indexes]\n",
    "    encrypted_chars = [vocab[idx] for idx in encrypted_indexes]\n",
    "    encrypted = ''.join(encrypted_chars)\n",
    "    return encrypted\n",
    "\n",
    "print(encrypt('RNN IS NOT AI', key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z6QNnDJPdxXC"
   },
   "source": [
    "Теперь нам необходимо нагенерировать датасет для решения задачи обучения с учителем. Нашим датасетом может быть случайно зашифрованные фразы, и тогда его структура будет следующей:\n",
    "message --- encrypted message\n",
    "\n",
    "Это пример параллельного корпуса из НЛП.\n",
    "\n",
    "Но нам необходимо представить каждую букву в виде ее номера в словаре, чтобы далее воспользоваться Embedding слоем. \n",
    "\n",
    "Для простоты давайте допустим, что все строки имеют одинаковую длину seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Te9ugR1AIjEw"
   },
   "outputs": [],
   "source": [
    "num_examples = 256 # размер датасета\n",
    "seq_len = 18 # максимальная длина строки\n",
    "\n",
    "def encrypted_dataset(dataset_len, k):\n",
    "    \"\"\"\n",
    "    Return: List(Tuple(Tensor encrypted, Tensor source))\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    for x in range(dataset_len):\n",
    "        random_message  = ''.join([random.choice(vocab) for x in range(seq_len)])\n",
    "        encrypt_random_message = encrypt(''.join(random_message), k)\n",
    "        src = [vocab.index(x) for x in random_message]\n",
    "        tgt = [vocab.index(x) for x in encrypt_random_message]\n",
    "        dataset.append([torch.tensor(tgt), torch.tensor(src)])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encrypted_dataset(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([17, 21, 20,  5, 10, 10, 27,  4,  8,  0,  0, 22, 22, 14,  0,  8, 13, 16]),\n",
       "  tensor([ 7, 11, 10, 23,  0,  0, 17, 22, 26, 18, 18, 12, 12,  4, 18, 26,  3,  6])],\n",
       " [tensor([ 2,  1,  3, 17,  0, 22, 18, 12, 25,  8,  8,  8, 10, 14, 14,  6,  9,  7]),\n",
       "  tensor([20, 19, 21,  7, 18, 12,  8,  2, 15, 26, 26, 26,  0,  4,  4, 24, 27, 25])],\n",
       " [tensor([ 1, 15, 25, 24, 13, 23,  7,  6, 24, 10,  9, 10, 19,  6,  9, 14, 26,  2]),\n",
       "  tensor([19,  5, 15, 14,  3, 13, 25, 24, 14,  0, 27,  0,  9, 24, 27,  4, 16, 20])],\n",
       " [tensor([26,  6, 17, 21, 19, 11, 27, 27, 27, 12, 18,  8, 23, 24, 17,  0,  5, 21]),\n",
       "  tensor([16, 24,  7, 11,  9,  1, 17, 17, 17,  2,  8, 26, 13, 14,  7, 18, 23, 11])],\n",
       " [tensor([ 4,  7, 11,  4, 24,  7, 10, 22,  0,  5,  7,  2,  4, 13, 19,  9, 15,  5]),\n",
       "  tensor([22, 25,  1, 22, 14, 25,  0, 12, 18, 23, 25, 20, 22,  3,  9, 27,  5, 23])],\n",
       " [tensor([18, 16, 25,  4, 21,  2, 13,  4, 22, 27, 21,  3, 24,  8, 23,  8, 11, 21]),\n",
       "  tensor([ 8,  6, 15, 22, 11, 20,  3, 22, 12, 17, 11, 21, 14, 26, 13, 26,  1, 11])],\n",
       " [tensor([19, 21, 27, 21, 10,  9, 24, 26, 27, 26, 25, 13, 24, 19,  8,  4,  5, 13]),\n",
       "  tensor([ 9, 11, 17, 11,  0, 27, 14, 16, 17, 16, 15,  3, 14,  9, 26, 22, 23,  3])],\n",
       " [tensor([14, 13, 19, 13,  5, 18, 19, 16,  5,  9, 26,  1, 26,  4, 13, 15, 13, 24]),\n",
       "  tensor([ 4,  3,  9,  3, 23,  8,  9,  6, 23, 27, 16, 19, 16, 22,  3,  5,  3, 14])],\n",
       " [tensor([24, 13, 26, 13, 12, 23, 17, 19,  5, 24, 13, 21, 24,  8, 15, 26, 19, 19]),\n",
       "  tensor([14,  3, 16,  3,  2, 13,  7,  9, 23, 14,  3, 11, 14, 26,  5, 16,  9,  9])],\n",
       " [tensor([ 2,  4,  4,  5, 10, 16,  1, 24, 25, 10,  3, 23,  5, 15, 22,  1,  0,  5]),\n",
       "  tensor([20, 22, 22, 23,  0,  6, 19, 14, 15,  0, 21, 13, 23,  5, 12, 19, 18, 23])]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0RUxQqTCGUtx"
   },
   "source": [
    "**Pytorch RNN:**\n",
    "$$h_t = \\text{tanh}(w_{ih} x_t + b_{ih} + w_{hh} h_{(t-1)} + b_{hh})$$\n",
    "\n",
    "**where : $h_t$ is the hidden state at time $t$, $x_t$ is\n",
    "    the input at time $t$, and $h_{(t-1)}$ is the hidden state of the\n",
    "    previous layer at time $t-1$ or the initial hidden state at time $0$.**\n",
    "    \n",
    "Args: \n",
    "\n",
    "        input_size: The number of expected features in the input $x$\n",
    "        hidden_size: The number of features in the hidden state $h$\n",
    "        num_layers: Number of recurrent layers. E.g., setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03y7VB9QLorQ"
   },
   "outputs": [],
   "source": [
    "class Decipher(nn.Module):\n",
    "    def __init__(self, vocab_size, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 rnn_type='simple'):\n",
    "        \"\"\"\n",
    "        :params: int vocab_size \n",
    "        :params: int embedding_dim\n",
    "        :params\n",
    "        \"\"\"\n",
    "        super(Decipher, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # look here: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "        if rnn_type == 'simple':\n",
    "            self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=2)\n",
    "         \n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.initial_hidden = torch.zeros(2, 1, hidden_dim)\n",
    "        \n",
    "    def forward(self, cipher):\n",
    "        # CHECK INPUT SIZE\n",
    "        # Unsqueeze 1 dimension for batches\n",
    "        # https://pytorch.org/docs/stable/torch.html\n",
    "        embd_x = self.embed(cipher).unsqueeze(1)\n",
    "        out_rnn, hidden = self.rnn(embd_x, self.initial_hidden)\n",
    "        # Apply the affine transform and transpose output in appropriate way\n",
    "        # because you want to get the softmax on vocabulary dimension\n",
    "        # in order to get probability of every letter\n",
    "        return self.fc(out_rnn).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rnb-2qIIjM7"
   },
   "outputs": [],
   "source": [
    "# Определим параметры нашей модели\n",
    "embedding_dim = 5\n",
    "hidden_dim = 10\n",
    "vocab_size = len(vocab) \n",
    "lr = 1e-3\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Инициализируем модель\n",
    "model = Decipher(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Инициализируем оптимизатор: рекомендуется Adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14680,
     "status": "ok",
     "timestamp": 1553359733380,
     "user": {
      "displayName": "Агата Шевченко",
      "photoUrl": "",
      "userId": "17622558543368228675"
     },
     "user_tz": -180
    },
    "id": "yZO1uR7fIjQ9",
    "outputId": "870713d3-1709-4e10-a32b-04408af00e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 2.8201\n",
      "Accuracy: 30.47%\n",
      "Epoch: 1\n",
      "Loss: 1.8440\n",
      "Accuracy: 60.46%\n",
      "Epoch: 2\n",
      "Loss: 1.3467\n",
      "Accuracy: 84.46%\n",
      "Epoch: 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-1b5535f364b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "for x in range(num_epochs):\n",
    "    print('Epoch: {}'.format(x))\n",
    "    for encrypted, original in encrypted_dataset(num_examples, k):\n",
    "\n",
    "        scores = model(encrypted)\n",
    "        original = original.unsqueeze(1)\n",
    "        # Calculate loss\n",
    "        loss = criterion(scores, original)\n",
    "        # Zero grads\n",
    "        optimizer.zero_grad()\n",
    "        # Backpropagate\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "    print('Loss: {:6.4f}'.format(loss.item()))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        matches, total = 0, 0\n",
    "        for encrypted, original in encrypted_dataset(num_examples, k):\n",
    "            # Compute a softmax over the outputs\n",
    "            predictions = torch.nn.functional.softmax(model(encrypted), 1)\n",
    "            # Choose the character with the maximum probability (greedy decoding)\n",
    "            _, batch_out = predictions.max(dim=1)\n",
    "            # Remove batch\n",
    "            batch_out = batch_out.squeeze(1)\n",
    "            # Calculate accuracy\n",
    "            matches += torch.eq(batch_out, original).sum().item()\n",
    "            total += torch.numel(batch_out)\n",
    "        accuracy = matches / total\n",
    "        print('Accuracy: {:4.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Part2, Seminar 2-blanks.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
